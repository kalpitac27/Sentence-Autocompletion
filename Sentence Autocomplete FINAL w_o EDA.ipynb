{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Autocompletion using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning-> Tokenize the cleaned data-> Split the data into train and test-> Model-> Training (50 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          PlayerLine\n",
      "0                                              ACT I\n",
      "1                       SCENE I. London. The palace.\n",
      "2  Enter KING HENRY, LORD JOHN OF LANCASTER, the ...\n",
      "3             So shaken as we are, so wan with care,\n",
      "4         Find we a time for frighted peace to pant,\n",
      "Sequence lengths (first 10): [2, 5, 16, 9, 9, 7, 7, 8, 9, 8]\n",
      "Max sequence length: 54\n",
      "Min sequence length: 1\n",
      "Number of non-empty sequences: 10000\n",
      "Max sequence length ->> 54\n",
      "Text Sequence ->>\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 455   4]\n",
      "Text Sequence Shape ->> (10000, 54)\n",
      "Sequence 0: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 455   4]\n",
      "Sequence 1: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0 138   4 241   2 589]\n",
      "Sequence 2: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  50  39  79  30 139   3 292   2 203   3 419 104 526 560   1 362]\n",
      "Sequence 3: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0   28 3904   20   36   44   28 3905   10  436]\n",
      "Sequence 4: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0  245   36    6  137   14 2647  161    5 3906]\n",
      "Padded sequences shape: (10000, 54)\n",
      "First Input:  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 455]\n",
      "First Output:  4\n",
      "Total number of words:  7865\n",
      "Input Shape:  (10000, 53)\n",
      "Output Shape:  (10000, 7865)\n",
      "Training Data Shapes:\n",
      "X_train: (8000, 53)\n",
      "y_train: (8000, 7865)\n",
      "Testing Data Shapes:\n",
      "X_test: (2000, 53)\n",
      "y_test: (2000, 7865)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalpi\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"LSTM_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"LSTM_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">786,500</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,255,424</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7865</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,034,745</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m100\u001b[0m)             │         \u001b[38;5;34m786,500\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m1,255,424\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7865\u001b[0m)                │       \u001b[38;5;34m4,034,745\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,076,669</span> (23.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,076,669\u001b[0m (23.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,076,669</span> (23.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,076,669\u001b[0m (23.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 281ms/step - accuracy: 0.0114 - loss: 8.2253 - val_accuracy: 0.0135 - val_loss: 7.6584\n",
      "Epoch 2/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 282ms/step - accuracy: 0.0171 - loss: 7.3341 - val_accuracy: 0.0165 - val_loss: 7.6608\n",
      "Epoch 3/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 284ms/step - accuracy: 0.0190 - loss: 7.0480 - val_accuracy: 0.0240 - val_loss: 7.7315\n",
      "Epoch 4/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 284ms/step - accuracy: 0.0285 - loss: 6.7547 - val_accuracy: 0.0245 - val_loss: 7.8293\n",
      "Epoch 5/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 277ms/step - accuracy: 0.0316 - loss: 6.4903 - val_accuracy: 0.0285 - val_loss: 8.0232\n",
      "Epoch 6/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 271ms/step - accuracy: 0.0388 - loss: 6.1631 - val_accuracy: 0.0295 - val_loss: 8.1547\n",
      "Epoch 7/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 275ms/step - accuracy: 0.0552 - loss: 5.7595 - val_accuracy: 0.0305 - val_loss: 8.3379\n",
      "Epoch 8/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 278ms/step - accuracy: 0.0735 - loss: 5.3286 - val_accuracy: 0.0270 - val_loss: 8.5189\n",
      "Epoch 9/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 274ms/step - accuracy: 0.1148 - loss: 4.8067 - val_accuracy: 0.0275 - val_loss: 8.7336\n",
      "Epoch 10/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 0.1709 - loss: 4.2421"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint at epoch 10 to path/to/checkpoint/model_epoch_10.h5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 274ms/step - accuracy: 0.1708 - loss: 4.2426 - val_accuracy: 0.0300 - val_loss: 8.9123\n",
      "Epoch 11/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 279ms/step - accuracy: 0.2425 - loss: 3.7069 - val_accuracy: 0.0250 - val_loss: 9.0578\n",
      "Epoch 12/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 283ms/step - accuracy: 0.3512 - loss: 3.1407 - val_accuracy: 0.0265 - val_loss: 9.3512\n",
      "Epoch 13/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 283ms/step - accuracy: 0.4535 - loss: 2.6220 - val_accuracy: 0.0250 - val_loss: 9.5717\n",
      "Epoch 14/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 281ms/step - accuracy: 0.5500 - loss: 2.1457 - val_accuracy: 0.0315 - val_loss: 9.7740\n",
      "Epoch 15/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 283ms/step - accuracy: 0.6442 - loss: 1.7479 - val_accuracy: 0.0265 - val_loss: 9.9929\n",
      "Epoch 16/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 283ms/step - accuracy: 0.7260 - loss: 1.4009 - val_accuracy: 0.0300 - val_loss: 10.1235\n",
      "Epoch 17/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 284ms/step - accuracy: 0.7918 - loss: 1.1489 - val_accuracy: 0.0270 - val_loss: 10.3407\n",
      "Epoch 18/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 275ms/step - accuracy: 0.8311 - loss: 0.9333 - val_accuracy: 0.0255 - val_loss: 10.4498\n",
      "Epoch 19/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 272ms/step - accuracy: 0.8550 - loss: 0.7799 - val_accuracy: 0.0285 - val_loss: 10.5130\n",
      "Epoch 20/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.8884 - loss: 0.6300"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint at epoch 20 to path/to/checkpoint/model_epoch_20.h5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 276ms/step - accuracy: 0.8884 - loss: 0.6302 - val_accuracy: 0.0295 - val_loss: 10.6854\n",
      "Epoch 21/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 275ms/step - accuracy: 0.9070 - loss: 0.5505 - val_accuracy: 0.0275 - val_loss: 10.8001\n",
      "Epoch 22/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 276ms/step - accuracy: 0.9076 - loss: 0.5074 - val_accuracy: 0.0305 - val_loss: 10.9251\n",
      "Epoch 23/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 277ms/step - accuracy: 0.9231 - loss: 0.4279 - val_accuracy: 0.0320 - val_loss: 10.9856\n",
      "Epoch 24/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 275ms/step - accuracy: 0.9237 - loss: 0.4104 - val_accuracy: 0.0290 - val_loss: 11.1049\n",
      "Epoch 25/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 275ms/step - accuracy: 0.9377 - loss: 0.3494 - val_accuracy: 0.0305 - val_loss: 11.1830\n",
      "Epoch 26/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 273ms/step - accuracy: 0.9404 - loss: 0.3368 - val_accuracy: 0.0320 - val_loss: 11.2471\n",
      "Epoch 27/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 274ms/step - accuracy: 0.9496 - loss: 0.2786 - val_accuracy: 0.0305 - val_loss: 11.2902\n",
      "Epoch 28/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 274ms/step - accuracy: 0.9476 - loss: 0.2739 - val_accuracy: 0.0300 - val_loss: 11.3259\n",
      "Epoch 29/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 272ms/step - accuracy: 0.9514 - loss: 0.2616 - val_accuracy: 0.0285 - val_loss: 11.3628\n",
      "Epoch 30/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.9499 - loss: 0.2558"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint at epoch 30 to path/to/checkpoint/model_epoch_30.h5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 273ms/step - accuracy: 0.9499 - loss: 0.2558 - val_accuracy: 0.0320 - val_loss: 11.4919\n",
      "Epoch 31/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 833ms/step - accuracy: 0.9545 - loss: 0.2408 - val_accuracy: 0.0295 - val_loss: 11.5246\n",
      "Epoch 32/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 272ms/step - accuracy: 0.9505 - loss: 0.2306 - val_accuracy: 0.0295 - val_loss: 11.6430\n",
      "Epoch 33/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 280ms/step - accuracy: 0.9570 - loss: 0.2112 - val_accuracy: 0.0290 - val_loss: 11.6834\n",
      "Epoch 34/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 278ms/step - accuracy: 0.9635 - loss: 0.1908 - val_accuracy: 0.0310 - val_loss: 11.7304\n",
      "Epoch 35/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 283ms/step - accuracy: 0.9555 - loss: 0.2127 - val_accuracy: 0.0315 - val_loss: 11.7086\n",
      "Epoch 36/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 279ms/step - accuracy: 0.9609 - loss: 0.1947 - val_accuracy: 0.0305 - val_loss: 11.8436\n",
      "Epoch 37/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 279ms/step - accuracy: 0.9555 - loss: 0.2091 - val_accuracy: 0.0310 - val_loss: 11.8258\n",
      "Epoch 38/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 279ms/step - accuracy: 0.9557 - loss: 0.2036 - val_accuracy: 0.0345 - val_loss: 11.8799\n",
      "Epoch 39/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 280ms/step - accuracy: 0.9576 - loss: 0.2004 - val_accuracy: 0.0320 - val_loss: 11.9206\n",
      "Epoch 40/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.9598 - loss: 0.1827"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint at epoch 40 to path/to/checkpoint/model_epoch_40.h5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 280ms/step - accuracy: 0.9598 - loss: 0.1827 - val_accuracy: 0.0315 - val_loss: 11.9757\n",
      "Epoch 41/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 278ms/step - accuracy: 0.9625 - loss: 0.1707 - val_accuracy: 0.0345 - val_loss: 12.0278\n",
      "Epoch 42/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 279ms/step - accuracy: 0.9585 - loss: 0.1753 - val_accuracy: 0.0320 - val_loss: 12.0479\n",
      "Epoch 43/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 277ms/step - accuracy: 0.9646 - loss: 0.1594 - val_accuracy: 0.0305 - val_loss: 12.0843\n",
      "Epoch 44/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 289ms/step - accuracy: 0.9612 - loss: 0.1704 - val_accuracy: 0.0315 - val_loss: 12.0360\n",
      "Epoch 45/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4139s\u001b[0m 17s/step - accuracy: 0.9589 - loss: 0.1797 - val_accuracy: 0.0310 - val_loss: 12.0910\n",
      "Epoch 46/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 284ms/step - accuracy: 0.9592 - loss: 0.1849 - val_accuracy: 0.0340 - val_loss: 12.0989\n",
      "Epoch 47/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 270ms/step - accuracy: 0.9556 - loss: 0.1816 - val_accuracy: 0.0270 - val_loss: 12.1177\n",
      "Epoch 48/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 274ms/step - accuracy: 0.9573 - loss: 0.1771 - val_accuracy: 0.0305 - val_loss: 12.1421\n",
      "Epoch 49/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 271ms/step - accuracy: 0.9612 - loss: 0.1680 - val_accuracy: 0.0290 - val_loss: 12.3166\n",
      "Epoch 50/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9637 - loss: 0.1570"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint at epoch 50 to path/to/checkpoint/model_epoch_50.h5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 276ms/step - accuracy: 0.9637 - loss: 0.1570 - val_accuracy: 0.0310 - val_loss: 12.2428\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('Shakespeare_data.csv')\n",
    "\n",
    "# Keep only 'PlayerLine' column\n",
    "data = data[['PlayerLine']]\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "# Extract the 'PlayerLine' column into a list\n",
    "text = data['PlayerLine'].tolist()\n",
    "\n",
    "# Define the text cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove special characters\n",
    "    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Remove digits\n",
    "    text = re.sub('\\d+', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Clean the text data\n",
    "texts = [clean_text(t) for t in text]\n",
    "\n",
    "# Limit to first 10000 texts\n",
    "texts = texts[:10000]\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts to sequences\n",
    "text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Check sequences length\n",
    "sequence_lengths = [len(seq) for seq in text_sequences]\n",
    "print(f\"Sequence lengths (first 10): {sequence_lengths[:10]}\")\n",
    "print(f\"Max sequence length: {max(sequence_lengths)}\")\n",
    "print(f\"Min sequence length: {min(sequence_lengths)}\")\n",
    "\n",
    "# Ensure sequences are not empty\n",
    "text_sequences = [seq for seq in text_sequences if len(seq) > 0]\n",
    "print(f\"Number of non-empty sequences: {len(text_sequences)}\")\n",
    "\n",
    "# Pad sequences to ensure they have the same length\n",
    "max_sequence_length = max(sequence_lengths)\n",
    "text_sequences = pad_sequences(text_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "print('Max sequence length ->>', max_sequence_length)\n",
    "print('Text Sequence ->>\\n', text_sequences[0])\n",
    "print('Text Sequence Shape ->>', text_sequences.shape)\n",
    "\n",
    "# Debugging sequences\n",
    "for i, seq in enumerate(text_sequences[:5]):\n",
    "    print(f\"Sequence {i}: {seq}\")\n",
    "\n",
    "# Check the shape of padded sequences\n",
    "print(f\"Padded sequences shape: {text_sequences.shape}\")\n",
    "\n",
    "# Splitting the dataset into input and output\n",
    "X, y = text_sequences[:, :-1], text_sequences[:, -1]\n",
    "print('First Input: ', X[0])\n",
    "print('First Output: ', y[0])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "total_words = len(word_index) + 1\n",
    "print('Total number of words: ', total_words)\n",
    "y = to_categorical(y, num_classes=total_words)\n",
    "\n",
    "print('Input Shape: ', X.shape)\n",
    "print('Output Shape: ', y.shape)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes to verify\n",
    "print('Training Data Shapes:')\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('Testing Data Shapes:')\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential(name=\"LSTM_Model\")\n",
    "\n",
    "# Adding embedding layer\n",
    "model.add(Embedding(input_dim=total_words, \n",
    "                    output_dim=100,  # Fixed dimension for embedding vectors\n",
    "                    input_shape=(max_sequence_length - 1,)))  # Use input_shape instead of input_length\n",
    "\n",
    "# Adding a LSTM layer\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Adding the final output activation with activation function of softmax\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Printing model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define the checkpoint callback to save every 10 epochs\n",
    "class CustomCheckpoint(Callback):\n",
    "    def __init__(self, save_path, save_freq):\n",
    "        super(CustomCheckpoint, self).__init__()\n",
    "        self.save_path = save_path\n",
    "        self.save_freq = save_freq\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.save_freq == 0:\n",
    "            save_filepath = self.save_path.format(epoch=epoch+1)\n",
    "            self.model.save(save_filepath)\n",
    "            print(f'Saved model checkpoint at epoch {epoch+1} to {save_filepath}')\n",
    "\n",
    "checkpoint_callback = CustomCheckpoint(\n",
    "    save_path='path/to/checkpoint/model_epoch_{epoch:02d}.h5',  # Path to save the model file\n",
    "    save_freq=10  # Save the model every 10 epochs\n",
    ")\n",
    "\n",
    "# Training the model with custom checkpoint callback\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, y_test),  # Use validation data tuple\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming 'tokenizer' is already defined and fitted on the text data\n",
    "tokenizer_path = 'path/to/tokenizer.pkl'  # Path where the tokenizer will be saved\n",
    "with open(tokenizer_path, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the tokenizer and model for inference and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "Predicted next word: durance\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_path = 'path/to/tokenizer.pkl'  # Path where the tokenizer is saved\n",
    "with open(tokenizer_path, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# Load the saved model\n",
    "saved_model_path = 'path/to/checkpoint/model_epoch_50.h5'  # Update this with the actual path\n",
    "model = tf.keras.models.load_model(saved_model_path)\n",
    "\n",
    "# Define max_sequence_length as it was during training\n",
    "max_sequence_length = 54  # Use the same max_sequence_length as used in training\n",
    "\n",
    "# Define a function to preprocess input text\n",
    "def preprocess_text(input_text, tokenizer, max_sequence_length):\n",
    "    cleaned_text = re.sub('[^a-zA-Z0-9\\s]', '', input_text).lower()\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length - 1, padding='pre')\n",
    "    return padded_sequence\n",
    "\n",
    "# Example input text for inference\n",
    "#input_text = \"Where valiant Talbot above human \" #thought #WRONG\n",
    "#input_text = \"All of one nature, of one substance \"#bred\n",
    "#input_text = \"Against acquaintance, kindred and \" #allies:\n",
    "input_text=\"is not a buff jerkin a most sweet robe of \" #durance?\n",
    "#input_text = \"A base Walloon, to win the Dauphin's \" #grace, #WRONG\n",
    "\n",
    "# Preprocess the input text\n",
    "input_sequence = preprocess_text(input_text, tokenizer, max_sequence_length)\n",
    "\n",
    "# Make a prediction\n",
    "predicted = model.predict(input_sequence)\n",
    "\n",
    "# Decode the prediction to get the word\n",
    "predicted_word_index = np.argmax(predicted, axis=-1)[0]\n",
    "predicted_word = tokenizer.index_word[predicted_word_index]\n",
    "\n",
    "print(f\"Predicted next word: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference and Evaluation using Perplexity Score and BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kalpi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This cardinal's shame\n",
      "This cardinal's shame mine\n",
      "This cardinal's shame mine blunt\n",
      "This cardinal's shame mine blunt gentleman\n",
      "This cardinal's shame mine blunt gentleman fall\n",
      "Completed Sentence: This cardinal's shame mine blunt gentleman fall\n",
      "Perplexity: 4.628903715023214\n",
      "BLEU Score: 3.940055059819774e-78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def autoCompletions(text, model, tokenizer, max_sequence_length, max_words, reference_text):\n",
    "    text_sequence = tokenizer.texts_to_sequences([text])\n",
    "    word_count = 0\n",
    "    total_log_prob = 0  # To accumulate log probabilities of predicted words\n",
    "\n",
    "    while word_count < max_words:\n",
    "        # Pad the current text sequence\n",
    "        padded_sequence = pad_sequences(text_sequence, maxlen=max_sequence_length - 1, padding='pre')\n",
    "        \n",
    "        # Predict probabilities for the next word\n",
    "        predictions = model.predict(padded_sequence, verbose=0)\n",
    "        \n",
    "        # Get the index of the word with the highest probability\n",
    "        y_pred_test = np.argmax(predictions)\n",
    "        \n",
    "        # Retrieve the predicted word corresponding to the index\n",
    "        predicted_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == y_pred_test:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        \n",
    "        # Append the predicted word to the text\n",
    "        text += \" \" + predicted_word\n",
    "        print(text)\n",
    "        word_count += 1\n",
    "        \n",
    "        # Update text_sequence with the new text\n",
    "        text_sequence = tokenizer.texts_to_sequences([text])\n",
    "        \n",
    "        # Compute log probability of the predicted word and accumulate it\n",
    "        predicted_word_prob = predictions[0][y_pred_test]\n",
    "        total_log_prob += np.log(predicted_word_prob + 1e-10)  # Adding a small epsilon to avoid log(0)\n",
    "    \n",
    "    # Compute perplexity\n",
    "    perplexity = np.exp(-total_log_prob / word_count)\n",
    "    \n",
    "    # Tokenize the reference text and the generated text\n",
    "    reference_tokens = nltk.word_tokenize(reference_text)\n",
    "    candidate_tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    bleu_score = sentence_bleu([reference_tokens], candidate_tokens)\n",
    "    \n",
    "    return text, perplexity, bleu_score\n",
    "\n",
    "# Example usage\n",
    "reference_text = \"This cardinal's more haughty than the devil.\"\n",
    "complete_sentence, perplexity, bleu_score = autoCompletions(\"This cardinal's\", model, tokenizer, max_sequence_length, 5, reference_text)\n",
    "print(\"Completed Sentence:\", complete_sentence)\n",
    "print(\"Perplexity:\", perplexity)\n",
    "print(\"BLEU Score:\", bleu_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Including smoothing to deal with 0 counts of 4-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kalpi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This cardinal's shame\n",
      "This cardinal's shame mine\n",
      "This cardinal's shame mine blunt\n",
      "This cardinal's shame mine blunt gentleman\n",
      "This cardinal's shame mine blunt gentleman fall\n",
      "Completed Sentence: This cardinal's shame mine blunt gentleman fall\n",
      "Perplexity: 4.628903715023214\n",
      "BLEU Score: 0.12131756417616475\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def autoCompletions(text, model, tokenizer, max_sequence_length, max_words, reference_text):\n",
    "    text_sequence = tokenizer.texts_to_sequences([text])\n",
    "    word_count = 0\n",
    "    total_log_prob = 0  # To accumulate log probabilities of predicted words\n",
    "\n",
    "    while word_count < max_words:\n",
    "        # Pad the current text sequence\n",
    "        padded_sequence = pad_sequences(text_sequence, maxlen=max_sequence_length - 1, padding='pre')\n",
    "        \n",
    "        # Predict probabilities for the next word\n",
    "        predictions = model.predict(padded_sequence, verbose=0)\n",
    "        \n",
    "        # Get the index of the word with the highest probability\n",
    "        y_pred_test = np.argmax(predictions)\n",
    "        \n",
    "        # Retrieve the predicted word corresponding to the index\n",
    "        predicted_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == y_pred_test:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        \n",
    "        # Append the predicted word to the text\n",
    "        text += \" \" + predicted_word\n",
    "        print(text)\n",
    "        word_count += 1\n",
    "        \n",
    "        # Update text_sequence with the new text\n",
    "        text_sequence = tokenizer.texts_to_sequences([text])\n",
    "        \n",
    "        # Compute log probability of the predicted word and accumulate it\n",
    "        predicted_word_prob = predictions[0][y_pred_test]\n",
    "        total_log_prob += np.log(predicted_word_prob + 1e-10)  # Adding a small epsilon to avoid log(0)\n",
    "    \n",
    "    # Compute perplexity\n",
    "    perplexity = np.exp(-total_log_prob / word_count)\n",
    "    \n",
    "    # Tokenize the reference text and the generated text\n",
    "    reference_tokens = nltk.word_tokenize(reference_text)\n",
    "    candidate_tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Compute BLEU score with smoothing\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    bleu_score = sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothing_function)\n",
    "    \n",
    "    return text, perplexity, bleu_score\n",
    "\n",
    "# Example usage\n",
    "reference_text = \"This cardinal's more haughty than the devil.\"\n",
    "complete_sentence, perplexity, bleu_score = autoCompletions(\"This cardinal's\", model, tokenizer, max_sequence_length, 5, reference_text)\n",
    "print(\"Completed Sentence:\", complete_sentence)\n",
    "print(\"Perplexity:\", perplexity)\n",
    "print(\"BLEU Score:\", bleu_score)\n",
    "\n",
    "#Low Perplexity: Indicates good next-word prediction but might suggest overfitting.\n",
    "#Low BLEU Score: Indicates the generated text doesn't match well with the reference text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
